[2022-02-07 10:43:02 cluster_patch1_cifar_multi_32] (main.py 362): INFO AMP_OPT_LEVEL: O1
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 256
  CACHE_MODE: part
  DATASET: cifar
  DATA_PATH: ../datasets/cifar10/
  IMG_SIZE: 32
  INTERPOLATION: bicubic
  NUM_WORKERS: 8
  PIN_MEMORY: true
EVAL_MODE: true
LOCAL_RANK: 4
MODEL:
  CLUSTER:
    K:
    - 32
    - 8
    - 1
    POS_DIM: 2
    POS_LAMBDA:
    - 2.0
    - 1.0
    - 0.5
    POS_MLP_BIAS: true
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: cluster_patch1_cifar_multi_32
  NUM_CLASSES: 1000
  RESUME: ''
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 8
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    PATCH_NORM: true
    PATCH_SIZE: 1
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    WINDOW_SIZE: 7
  TYPE: cluster
OUTPUT: output/cluster_patch1_cifar_multi_32/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 0.002
  CLIP_GRAD: 5.0
  EPOCHS: 1000
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MIN_LR: 2.0e-05
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 2.0e-06
  WEIGHT_DECAY: 0.05

[2022-02-07 10:43:03 cluster_patch1_cifar_multi_32] (main.py 84): INFO Creating model:cluster/cluster_patch1_cifar_multi_32
[2022-02-07 10:43:05 cluster_patch1_cifar_multi_32] (main.py 88): INFO ClusterTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 96, kernel_size=(1, 1), stride=(1, 1))
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=96, depth=2
      (blocks): ModuleList(
        (0): ClusterTransformerBlock(
          dim=96, num_heads=3, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): ClusterAttention(
            dim=96, num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (pos_mlp): Linear(in_features=2, out_features=3, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): ClusterTransformerBlock(
          dim=96, num_heads=3, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): ClusterAttention(
            dim=96, num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (pos_mlp): Linear(in_features=2, out_features=3, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        dim=96
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (reduction): Linear(in_features=384, out_features=192, bias=False)
      )
    )
    (1): BasicLayer(
      dim=192, depth=2
      (blocks): ModuleList(
        (0): ClusterTransformerBlock(
          dim=192, num_heads=6, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): ClusterAttention(
            dim=192, num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (pos_mlp): Linear(in_features=2, out_features=6, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): ClusterTransformerBlock(
          dim=192, num_heads=6, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): ClusterAttention(
            dim=192, num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (pos_mlp): Linear(in_features=2, out_features=6, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        dim=192
        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (reduction): Linear(in_features=768, out_features=384, bias=False)
      )
    )
    (2): BasicLayer(
      dim=384, depth=8
      (blocks): ModuleList(
        (0): ClusterTransformerBlock(
          dim=384, num_heads=12, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): ClusterAttention(
            dim=384, num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (pos_mlp): Linear(in_features=2, out_features=12, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): ClusterTransformerBlock(
          dim=384, num_heads=12, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): ClusterAttention(
            dim=384, num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (pos_mlp): Linear(in_features=2, out_features=12, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): ClusterTransformerBlock(
          dim=384, num_heads=12, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): ClusterAttention(
            dim=384, num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (pos_mlp): Linear(in_features=2, out_features=12, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): ClusterTransformerBlock(
          dim=384, num_heads=12, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): ClusterAttention(
            dim=384, num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (pos_mlp): Linear(in_features=2, out_features=12, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): ClusterTransformerBlock(
          dim=384, num_heads=12, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): ClusterAttention(
            dim=384, num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (pos_mlp): Linear(in_features=2, out_features=12, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): ClusterTransformerBlock(
          dim=384, num_heads=12, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): ClusterAttention(
            dim=384, num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (pos_mlp): Linear(in_features=2, out_features=12, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): ClusterTransformerBlock(
          dim=384, num_heads=12, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): ClusterAttention(
            dim=384, num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (pos_mlp): Linear(in_features=2, out_features=12, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): ClusterTransformerBlock(
          dim=384, num_heads=12, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): ClusterAttention(
            dim=384, num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (pos_mlp): Linear(in_features=2, out_features=12, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=384, out_features=10, bias=True)
)
[2022-02-07 10:43:05 cluster_patch1_cifar_multi_32] (main.py 97): INFO number of params: 15685600
[2022-02-07 10:43:05 cluster_patch1_cifar_multi_32] (main.py 100): INFO number of GFLOPs: 0.0
[2022-02-07 10:43:05 cluster_patch1_cifar_multi_32] (main.py 122): INFO auto resuming from output/cluster_patch1_cifar_multi_32/default/ckpt_epoch.pth
[2022-02-07 10:43:05 cluster_patch1_cifar_multi_32] (utils.py 20): INFO ==============> Resuming form output/cluster_patch1_cifar_multi_32/default/ckpt_epoch.pth....................
[2022-02-07 10:43:05 cluster_patch1_cifar_multi_32] (utils.py 27): INFO <All keys matched successfully>
[2022-02-07 10:44:43 cluster_patch1_cifar_multi_32] (main.py 362): INFO AMP_OPT_LEVEL: O1
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: cifar
  DATA_PATH: ../datasets/cifar10/
  IMG_SIZE: 32
  INTERPOLATION: bicubic
  NUM_WORKERS: 8
  PIN_MEMORY: true
EVAL_MODE: true
LOCAL_RANK: 4
MODEL:
  CLUSTER:
    K:
    - 32
    - 8
    - 1
    POS_DIM: 2
    POS_LAMBDA:
    - 2.0
    - 1.0
    - 0.5
    POS_MLP_BIAS: true
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: cluster_patch1_cifar_multi_32
  NUM_CLASSES: 1000
  RESUME: ''
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 8
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    PATCH_NORM: true
    PATCH_SIZE: 1
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    WINDOW_SIZE: 7
  TYPE: cluster
OUTPUT: output/cluster_patch1_cifar_multi_32/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 0.001
  CLIP_GRAD: 5.0
  EPOCHS: 1000
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MIN_LR: 1.0e-05
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.0e-06
  WEIGHT_DECAY: 0.05

[2022-02-07 10:44:45 cluster_patch1_cifar_multi_32] (main.py 84): INFO Creating model:cluster/cluster_patch1_cifar_multi_32
[2022-02-07 10:44:45 cluster_patch1_cifar_multi_32] (main.py 88): INFO ClusterTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 96, kernel_size=(1, 1), stride=(1, 1))
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=96, depth=2
      (blocks): ModuleList(
        (0): ClusterTransformerBlock(
          dim=96, num_heads=3, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): ClusterAttention(
            dim=96, num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (pos_mlp): Linear(in_features=2, out_features=3, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): ClusterTransformerBlock(
          dim=96, num_heads=3, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): ClusterAttention(
            dim=96, num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (pos_mlp): Linear(in_features=2, out_features=3, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        dim=96
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (reduction): Linear(in_features=384, out_features=192, bias=False)
      )
    )
    (1): BasicLayer(
      dim=192, depth=2
      (blocks): ModuleList(
        (0): ClusterTransformerBlock(
          dim=192, num_heads=6, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): ClusterAttention(
            dim=192, num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (pos_mlp): Linear(in_features=2, out_features=6, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): ClusterTransformerBlock(
          dim=192, num_heads=6, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): ClusterAttention(
            dim=192, num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (pos_mlp): Linear(in_features=2, out_features=6, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        dim=192
        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (reduction): Linear(in_features=768, out_features=384, bias=False)
      )
    )
    (2): BasicLayer(
      dim=384, depth=8
      (blocks): ModuleList(
        (0): ClusterTransformerBlock(
          dim=384, num_heads=12, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): ClusterAttention(
            dim=384, num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (pos_mlp): Linear(in_features=2, out_features=12, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): ClusterTransformerBlock(
          dim=384, num_heads=12, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): ClusterAttention(
            dim=384, num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (pos_mlp): Linear(in_features=2, out_features=12, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): ClusterTransformerBlock(
          dim=384, num_heads=12, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): ClusterAttention(
            dim=384, num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (pos_mlp): Linear(in_features=2, out_features=12, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): ClusterTransformerBlock(
          dim=384, num_heads=12, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): ClusterAttention(
            dim=384, num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (pos_mlp): Linear(in_features=2, out_features=12, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): ClusterTransformerBlock(
          dim=384, num_heads=12, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): ClusterAttention(
            dim=384, num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (pos_mlp): Linear(in_features=2, out_features=12, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): ClusterTransformerBlock(
          dim=384, num_heads=12, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): ClusterAttention(
            dim=384, num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (pos_mlp): Linear(in_features=2, out_features=12, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): ClusterTransformerBlock(
          dim=384, num_heads=12, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): ClusterAttention(
            dim=384, num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (pos_mlp): Linear(in_features=2, out_features=12, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): ClusterTransformerBlock(
          dim=384, num_heads=12, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): ClusterAttention(
            dim=384, num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (pos_mlp): Linear(in_features=2, out_features=12, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=384, out_features=10, bias=True)
)
[2022-02-07 10:44:46 cluster_patch1_cifar_multi_32] (main.py 97): INFO number of params: 15685600
[2022-02-07 10:44:46 cluster_patch1_cifar_multi_32] (main.py 100): INFO number of GFLOPs: 0.0
[2022-02-07 10:44:46 cluster_patch1_cifar_multi_32] (main.py 122): INFO auto resuming from output/cluster_patch1_cifar_multi_32/default/ckpt_epoch.pth
[2022-02-07 10:44:46 cluster_patch1_cifar_multi_32] (utils.py 20): INFO ==============> Resuming form output/cluster_patch1_cifar_multi_32/default/ckpt_epoch.pth....................
[2022-02-07 10:44:46 cluster_patch1_cifar_multi_32] (utils.py 27): INFO <All keys matched successfully>
[2022-02-07 10:54:44 cluster_patch1_cifar_multi_32] (main.py 362): INFO AMP_OPT_LEVEL: O1
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: cifar
  DATA_PATH: ../datasets/cifar10/
  IMG_SIZE: 32
  INTERPOLATION: bicubic
  NUM_WORKERS: 8
  PIN_MEMORY: true
EVAL_MODE: true
LOCAL_RANK: 4
MODEL:
  CLUSTER:
    K:
    - 32
    - 8
    - 1
    POS_DIM: 2
    POS_LAMBDA:
    - 2.0
    - 1.0
    - 0.5
    POS_MLP_BIAS: true
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: cluster_patch1_cifar_multi_32
  NUM_CLASSES: 1000
  RESUME: ''
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 8
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    PATCH_NORM: true
    PATCH_SIZE: 1
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    WINDOW_SIZE: 7
  TYPE: cluster
OUTPUT: output/cluster_patch1_cifar_multi_32/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 0.001
  CLIP_GRAD: 5.0
  EPOCHS: 1000
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MIN_LR: 1.0e-05
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.0e-06
  WEIGHT_DECAY: 0.05

[2022-02-07 10:54:46 cluster_patch1_cifar_multi_32] (main.py 84): INFO Creating model:cluster/cluster_patch1_cifar_multi_32
[2022-02-07 10:54:46 cluster_patch1_cifar_multi_32] (main.py 88): INFO ClusterTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 96, kernel_size=(1, 1), stride=(1, 1))
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=96, depth=2
      (blocks): ModuleList(
        (0): ClusterTransformerBlock(
          dim=96, num_heads=3, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): ClusterAttention(
            dim=96, num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (pos_mlp): Linear(in_features=2, out_features=3, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): ClusterTransformerBlock(
          dim=96, num_heads=3, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): ClusterAttention(
            dim=96, num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (pos_mlp): Linear(in_features=2, out_features=3, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        dim=96
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (reduction): Linear(in_features=384, out_features=192, bias=False)
      )
    )
    (1): BasicLayer(
      dim=192, depth=2
      (blocks): ModuleList(
        (0): ClusterTransformerBlock(
          dim=192, num_heads=6, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): ClusterAttention(
            dim=192, num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (pos_mlp): Linear(in_features=2, out_features=6, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): ClusterTransformerBlock(
          dim=192, num_heads=6, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): ClusterAttention(
            dim=192, num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (pos_mlp): Linear(in_features=2, out_features=6, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        dim=192
        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (reduction): Linear(in_features=768, out_features=384, bias=False)
      )
    )
    (2): BasicLayer(
      dim=384, depth=8
      (blocks): ModuleList(
        (0): ClusterTransformerBlock(
          dim=384, num_heads=12, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): ClusterAttention(
            dim=384, num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (pos_mlp): Linear(in_features=2, out_features=12, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): ClusterTransformerBlock(
          dim=384, num_heads=12, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): ClusterAttention(
            dim=384, num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (pos_mlp): Linear(in_features=2, out_features=12, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): ClusterTransformerBlock(
          dim=384, num_heads=12, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): ClusterAttention(
            dim=384, num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (pos_mlp): Linear(in_features=2, out_features=12, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): ClusterTransformerBlock(
          dim=384, num_heads=12, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): ClusterAttention(
            dim=384, num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (pos_mlp): Linear(in_features=2, out_features=12, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): ClusterTransformerBlock(
          dim=384, num_heads=12, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): ClusterAttention(
            dim=384, num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (pos_mlp): Linear(in_features=2, out_features=12, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): ClusterTransformerBlock(
          dim=384, num_heads=12, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): ClusterAttention(
            dim=384, num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (pos_mlp): Linear(in_features=2, out_features=12, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): ClusterTransformerBlock(
          dim=384, num_heads=12, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): ClusterAttention(
            dim=384, num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (pos_mlp): Linear(in_features=2, out_features=12, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): ClusterTransformerBlock(
          dim=384, num_heads=12, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): ClusterAttention(
            dim=384, num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (pos_mlp): Linear(in_features=2, out_features=12, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=384, out_features=10, bias=True)
)
[2022-02-07 10:54:47 cluster_patch1_cifar_multi_32] (main.py 97): INFO number of params: 15685600
[2022-02-07 10:54:47 cluster_patch1_cifar_multi_32] (main.py 100): INFO number of GFLOPs: 0.0
[2022-02-07 10:54:47 cluster_patch1_cifar_multi_32] (main.py 122): INFO auto resuming from output/cluster_patch1_cifar_multi_32/default/ckpt_epoch.pth
[2022-02-07 10:54:47 cluster_patch1_cifar_multi_32] (utils.py 20): INFO ==============> Resuming form output/cluster_patch1_cifar_multi_32/default/ckpt_epoch.pth....................
[2022-02-07 10:54:47 cluster_patch1_cifar_multi_32] (utils.py 27): INFO <All keys matched successfully>
[2022-02-07 10:55:00 cluster_patch1_cifar_multi_32] (main.py 284): INFO Test: [0/10]	Time 12.781 (12.781)	Loss 0.6606 (0.6606)	Acc@1 81.348 (81.348)	Acc@5 98.535 (98.535)	Mem 12095MB
[2022-02-07 10:55:10 cluster_patch1_cifar_multi_32] (main.py 291): INFO  * Acc@1 81.530 Acc@5 98.640
[2022-02-07 10:55:10 cluster_patch1_cifar_multi_32] (main.py 129): INFO Accuracy of the network on the 10000 test images: 81.5%
